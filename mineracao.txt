# Mineração de Dados - Resumo Completo e Detalhado

## 1. Sistema de Informação (SI)

**Definição:** Um Sistema de Informação coleta, processa, armazena, analisa e dissemina informações para um objetivo específico, apoiando processos de decisão e operações organizacionais.

**Exemplo prático:** Um sistema bancário que processa transações financeiras, analisa riscos de crédito e gera relatórios gerenciais para clientes e gestores, integrando dados de múltiplas fontes para suportar decisões estratégicas.

## 2. O que é Mineração de Dados?

**Definição:** É o processo de descobrir automaticamente padrões, relações, anomalias e informação útil em grandes repositórios de dados, utilizando algoritmos estatísticos, aprendizado de máquina e inteligência artificial.

**Objetivo:** Transformar dados brutos em conhecimento acionável para suportar decisões estratégicas, prever tendências futuras e otimizar operações organizacionais.

**Exemplos práticos:**

\begin{itemize}
\item Identificar padrões de compra em clientes para recomendar novos produtos em e-commerce
\item Detectar fraudes em transações bancárias analisando comportamentos atípicos
\item Prever demanda de produtos para otimizar estoques em varejo
\end{itemize}

## 3. KDD - Knowledge Discovery in Databases

**Definição:** É o processo geral e sistemático de descoberta de conhecimento em bases de dados, do qual a mineração de dados é uma etapa fundamental, mas não a única.

**Etapas do Processo KDD**

\begin{enumerate}
\item \textbf{Seleção:} Identificação e coleta dos dados relevantes para o problema
\item \textbf{Pré-processamento:} Limpeza dos dados, tratamento de valores ausentes e redução de ruídos
\item \textbf{Transformação:} Conversão dos dados para formatos adequados (normalização, agregação, discretização)
\item \textbf{Mineração de Dados:} Aplicação de algoritmos específicos para identificar padrões significativos
\item \textbf{Interpretação e Avaliação:} Compreensão, validação e apresentação dos resultados gerados
\end{enumerate}

**Exemplo completo:** Em um hospital, o processo KDD envolveria: (1) coletar prontuários eletrônicos de pacientes, (2) limpar inconsistências e erros de digitação, (3) transformar escalas de medidas para unidades padronizadas, (4) aplicar algoritmos para buscar padrões de evolução de doenças, (5) validar resultados com especialistas médicos e apresentar insights acionáveis.

## 4. Dados de Entrada

**Armazenamento:** Os dados podem ser armazenados em uma grande variedade de formatos e sistemas:

\begin{enumerate}
\item Bancos de dados relacionais (MySQL, PostgreSQL, Oracle)
\item Planilhas eletrônicas (Excel, Google Sheets)
\item Bancos NoSQL (MongoDB, Cassandra)
\item Arquivos texto estruturados (CSV, JSON, XML)
\item Data warehouses e data lakes
\item Sistemas de big data (Hadoop, Spark)
\end{enumerate}

**Exemplos práticos:**

\begin{itemize}
\item Informações de vendas armazenadas em planilhas Excel
\item Logs de acesso a servidores web em arquivos texto
\item Dados de sensores IoT em bancos de dados de séries temporais
\item Transações financeiras em sistemas OLTP relacionais
\end{itemize}

## 5. Pré-processamento de Dados

**Definição:** Etapa que visa colocar os dados de entrada em formato apropriado e com qualidade suficiente para análise subsequente. É considerada **a etapa mais trabalhosa** do processo de mineração, consumindo frequentemente 60-80% do tempo total do projeto.

**Atividades Principais do Pré-processamento**

\begin{table}
\begin{tabular}{|l|p{10cm}|}
\hline
\textbf{Atividade} & \textbf{Descrição} \\
\hline
Integração & Junção e combinação de dados vindos de diversas fontes heterogêneas \\
\hline
Limpeza & Remoção de registros duplicados, tratamento de ruídos e correção de inconsistências \\
\hline
Seleção & Escolha manual ou automática de registros e atributos relevantes para a tarefa \\
\hline
Transformação & Conversão de formatos, normalização e padronização de valores \\
\hline
Redução & Diminuição do volume de dados mantendo informação essencial \\
\hline
\end{tabular}
\caption{Atividades principais do pré-processamento de dados}
\end{table}

**Exemplos práticos:**

\begin{itemize}
\item \textbf{Integração:} Combinar dados de vendas online com dados de lojas físicas
\item \textbf{Limpeza:} Remover notas duplicadas de alunos, padronizar nomes em letras maiúsculas
\item \textbf{Seleção:} Escolher apenas clientes ativos nos últimos 12 meses para análise
\item \textbf{Transformação:} Converter datas de formatos diferentes para padrão ISO
\item \textbf{Redução:} Agregar vendas diárias em vendas mensais para análise de tendências
\end{itemize}

**Importância:** A etapa de pré-processamento busca assegurar a qualidade dos dados para que o resultado final seja o melhor possível. O princípio é: "garbage in, garbage out" - dados ruins produzem resultados ruins.

## 6. Tarefas em Mineração de Dados

Normalmente, as tarefas de mineração de dados são classificadas em duas categorias principais:

### a) Tarefas Preditivas

**Definição:** Almejam prever o valor de um atributo alvo ("target" ou variável dependente) particular baseado nos valores de outros atributos (explicativos, independentes ou preditores).

**Principais técnicas:**

\begin{enumerate}
\item \textbf{Classificação:} Prever categoria ou classe (exemplo: cliente vai ou não inadimplir?)
\item \textbf{Regressão:} Prever valor numérico contínuo (exemplo: qual será o preço de venda de um imóvel?)
\end{enumerate}

**Exemplos práticos:**

\begin{itemize}
\item Prever inadimplência de clientes com base em renda, idade e histórico de crédito
\item Estimar o valor de mercado de imóveis usando localização, tamanho e características
\item Prever abandono de clientes (churn) em empresas de telecomunicações
\item Diagnosticar doenças baseado em sintomas e exames laboratoriais
\end{itemize}

### b) Tarefas Descritivas

**Definição:** Almejam derivar padrões (correlações, agrupamentos, tendências, anomalias) que resumam as relações subjacentes nos dados. Geralmente são exploratórias e frequentemente requerem técnicas de pós-processamento para validar e explicar os resultados.

**Principais técnicas:**

\begin{enumerate}
\item \textbf{Agrupamento (Clustering):} Encontrar grupos naturais nos dados
\item \textbf{Regras de Associação:} Descobrir relações frequentes entre itens
\item \textbf{Detecção de Anomalias:} Identificar padrões incomuns ou suspeitos
\item \textbf{Sumarização:} Criar resumos compactos e informativos dos dados
\end{enumerate}

**Exemplos práticos:**

\begin{itemize}
\item Segmentar consumidores em grupos com comportamentos similares (marketing)
\item Encontrar produtos frequentemente comprados juntos (análise de cesta de mercado)
\item Detectar transações fraudulentas em cartões de crédito
\item Identificar tópicos principais em grandes volumes de documentos
\end{itemize}

## 7. Tipos de Dados

### a) Atributo (ou Característica)

**Definição:** É uma propriedade ou característica de um objeto que pode variar, seja de um objeto para outro, ou de um tempo para outro.

**Exemplos:** Altura de uma pessoa, cor de um carro, temperatura ambiente, saldo bancário.

### b) Medida de Escala

**Definição:** É uma função ou regra que associa um valor numérico ou simbólico ao atributo de um objeto (pesar, contar, classificar, ordenar).

**Exemplos:**

\begin{itemize}
\item Medir altura em centímetros (numérico)
\item Contar número de páginas de um livro (numérico discreto)
\item Classificar cor do carro (categórico nominal)
\end{itemize}

## 8. Tipos de Atributo

Os atributos podem ser classificados segundo diferentes critérios:

### Classificação Principal

**Atributos Categóricos:**

\begin{enumerate}
\item \textbf{Nominal:} Não possuem ordem implícita entre os valores. Servem apenas para identificar categorias distintas.
\begin{itemize}
\item Exemplos: cor dos olhos (azul, verde, castanho), estado civil (solteiro, casado, divorciado), tipo sanguíneo (A, B, AB, O)
\end{itemize}

\item \textbf{Ordinal:} Possuem ordem ou hierarquia natural entre os valores, mas a distância entre eles não é uniforme ou mensurável.
\begin{itemize}
\item Exemplos: nível de escolaridade (fundamental, médio, superior), avaliação de serviço (ruim, regular, bom, ótimo), tamanho de roupa (P, M, G, GG)
\end{itemize}
\end{enumerate}

**Atributos Numéricos:**

\begin{enumerate}
\item \textbf{Intervalar:} Diferença entre valores faz sentido, mas não existe "zero absoluto" que indique ausência total da propriedade. Operações de soma e subtração são válidas, mas não multiplicação ou divisão.
\begin{itemize}
\item Exemplos: temperatura em Celsius ou Fahrenheit (0°C não significa ausência de calor), ano calendário (ano 0 é convenção)
\end{itemize}

\item \textbf{Razão (ou Frequência):} Diferença e proporção fazem sentido, existe "zero absoluto" que representa ausência completa. Todas as operações matemáticas são válidas.
\begin{itemize}
\item Exemplos: salário (R\$ 0 = sem salário), idade (0 anos = recém-nascido), altura, peso, número de vendas, distância
\end{itemize}
\end{enumerate}

### Classificação por Natureza dos Valores

\begin{enumerate}
\item \textbf{Discreto:} Assume valores contáveis e finitos (ou infinitos enumeráveis)
\begin{itemize}
\item Exemplos: número de filhos, quantidade de carros, número de transações
\end{itemize}

\item \textbf{Contínuo:} Pode assumir qualquer valor em um intervalo (infinitos não-enumeráveis)
\begin{itemize}
\item Exemplos: altura, peso, temperatura, tempo de resposta
\end{itemize}
\end{enumerate}

## 9. Medidas de Similaridade e Dissimilaridade

**Definição:** Medidas de similaridade e dissimilaridade são fundamentais em mineração de dados para comparar objetos, identificar padrões e agrupar dados semelhantes. Enquanto **similaridade** quantifica o quão parecidos são dois objetos (valores maiores indicam maior semelhança), **dissimilaridade** ou **distância** quantifica o quão diferentes são (valores maiores indicam maior diferença)[1][2].

### Conceitos Fundamentais

**Similaridade:**
\begin{itemize}
\item Mede o grau de semelhança entre dois objetos
\item Valores próximos de 1 (ou máximo) indicam alta similaridade
\item $s(p, q) = 1$ (ou máximo) apenas se $p = q$
\item $s(p, q) = s(q, p)$ (propriedade simétrica)
\end{itemize}

**Dissimilaridade:**
\begin{itemize}
\item Mede o grau de diferença entre dois objetos
\item Valores próximos de 0 indicam baixa dissimilaridade (alta similaridade)
\item $d(p, q) = 0$ apenas se $p = q$
\item $d(p, q) = d(q, p)$ (propriedade simétrica)
\item $d(p, q) \geq 0$ (não-negatividade)
\end{itemize}

**Relação entre Similaridade e Dissimilaridade:**

Frequentemente, podemos converter entre essas medidas:

$$s(p, q) = \frac{1}{1 + d(p, q)}$$

ou

$$d(p, q) = 1 - s(p, q)$$

### Principais Medidas de Distância

#### 1. Distância Euclidiana

**Definição:** É a medida de distância mais comum e intuitiva, calculando a "linha reta" entre dois pontos no espaço multidimensional[1][2].

**Fórmula:** Para dois objetos $i$ e $j$ com $p$ atributos:

$$d_E(i, j) = \sqrt{\sum_{k=1}^{p}(x_{ik} - x_{jk})^2}$$

**Exemplo prático:**

Considere dois clientes com atributos [idade, renda]:
\begin{itemize}
\item Cliente A: [25, 3000]
\item Cliente B: [30, 3500]
\end{itemize}

$$d_E(A, B) = \sqrt{(25-30)^2 + (3000-3500)^2} = \sqrt{25 + 250000} = \sqrt{250025} \approx 500.02$$

**Características:**
\begin{itemize}
\item Apropriada para atributos numéricos contínuos
\item Sensível à escala dos atributos (normalização é recomendada)
\item Satisfaz propriedades de métrica (não-negatividade, simetria, desigualdade triangular)
\end{itemize}

#### 2. Distância Euclidiana Ponderada

**Definição:** Versão da distância Euclidiana que atribui pesos diferentes aos atributos, permitindo enfatizar características mais importantes[1].

**Fórmula:**

$$d_{WE}(i, j) = \sqrt{\sum_{k=1}^{p}w_k(x_{ik} - x_{jk})^2}$$

onde $w_k$ é o peso do atributo $k$.

**Exemplo:** Se a idade é 3 vezes mais importante que a renda:
\begin{itemize}
\item Pesos: $w_1 = 3$ (idade), $w_2 = 1$ (renda)
\item $d_{WE}(A, B) = \sqrt{3(25-30)^2 + 1(3000-3500)^2} = \sqrt{75 + 250000} \approx 500.07$
\end{itemize}

#### 3. Distância Manhattan (ou City Block)

**Definição:** Calcula a distância como a soma das diferenças absolutas em cada dimensão, representando o caminho percorrido ao mover-se apenas horizontal e verticalmente (como em uma grade urbana)[2].

**Fórmula:**

$$d_M(i, j) = \sum_{k=1}^{p}|x_{ik} - x_{jk}|$$

**Exemplo:**

Para os clientes A e B:

$$d_M(A, B) = |25-30| + |3000-3500| = 5 + 500 = 505$$

**Características:**
\begin{itemize}
\item Menos sensível a outliers que a distância Euclidiana
\item Útil quando movimentos são restritos a eixos (como ruas de cidade)
\item Computacionalmente mais simples que Euclidiana
\end{itemize}

#### 4. Distância Minkowski

**Definição:** Generalização das distâncias Euclidiana e Manhattan[2].

**Fórmula:**

$$d_{Minkowski}(i, j) = \left(\sum_{k=1}^{p}|x_{ik} - x_{jk}|^r\right)^{1/r}$$

onde:
\begin{itemize}
\item $r = 1$: Distância Manhattan
\item $r = 2$: Distância Euclidiana
\item $r = \infty$: Distância Chebyshev (máximo)
\end{itemize}

#### 5. Distância de Chebyshev

**Definição:** Considera apenas a maior diferença entre as dimensões[2].

**Fórmula:**

$$d_{Chebyshev}(i, j) = \max_{k}|x_{ik} - x_{jk}|$$

**Exemplo:**

Para os clientes A e B:

$$d_{Chebyshev}(A, B) = \max(|25-30|, |3000-3500|) = \max(5, 500) = 500$$

#### 6. Distância Mahalanobis

**Definição:** Considera a correlação entre variáveis e é invariante a mudanças de escala[2][3].

**Fórmula:**

$$d_{Mahalanobis}(x, y) = \sqrt{(x-y)^T S^{-1} (x-y)}$$

onde $S$ é a matriz de covariância.

**Vantagens:**
\begin{itemize}
\item Leva em conta a correlação entre variáveis
\item Não requer normalização prévia
\item Útil para detectar outliers multivariados
\end{itemize}

#### 7. Distância de Hamming

**Definição:** Utilizada para dados categóricos ou binários, conta o número de posições em que os valores diferem[2].

**Fórmula:**

$$d_{Hamming}(x, y) = \sum_{k=1}^{p} \mathbb{1}_{x_k \neq y_k}$$

onde $\mathbb{1}_{x_k \neq y_k}$ é 1 se $x_k \neq y_k$ e 0 caso contrário.

**Exemplo:**

Para strings binárias:
\begin{itemize}
\item String A: [1, 0, 1, 1, 0]
\item String B: [1, 1, 1, 0, 0]
\item $d_{Hamming}(A, B) = 2$ (diferem nas posições 2 e 4)
\end{itemize}

### Medidas para Dados Categóricos

#### Coeficiente de Jaccard

**Definição:** Mede similaridade entre conjuntos, útil para dados binários ou presença/ausência.

**Fórmula:**

$$J(A, B) = \frac{|A \cap B|}{|A \cup B|}$$

**Exemplo:** Produtos comprados por dois clientes:
\begin{itemize}
\item Cliente 1: \{Arroz, Feijão, Açúcar\}
\item Cliente 2: \{Arroz, Café, Açúcar\}
\item $J = \frac{2}{4} = 0.5$ (2 produtos em comum, 4 produtos únicos no total)
\end{itemize}

#### Coeficiente de Dice (Sørensen-Dice)

**Fórmula:**

$$Dice(A, B) = \frac{2|A \cap B|}{|A| + |B|}$$

Dá mais peso aos elementos em comum.

### Medidas de Similaridade Cossenoidal

**Definição:** Mede o cosseno do ângulo entre dois vetores, útil para dados textuais e esparsos[2].

**Fórmula:**

$$cos(\theta) = \frac{x \cdot y}{||x|| \cdot ||y||} = \frac{\sum_{i=1}^{p} x_i y_i}{\sqrt{\sum_{i=1}^{p} x_i^2} \cdot \sqrt{\sum_{i=1}^{p} y_i^2}}$$

**Características:**
\begin{itemize}
\item Valores entre -1 (totalmente opostos) e 1 (idênticos)
\item Ignora magnitude, considera apenas direção
\item Amplamente usado em análise de texto e sistemas de recomendação
\end{itemize}

**Exemplo:** Frequência de palavras em documentos:
\begin{itemize}
\item Documento 1: [3, 2, 0, 5]
\item Documento 2: [4, 3, 0, 6]
\item $cos(\theta) = \frac{3 \cdot 4 + 2 \cdot 3 + 0 \cdot 0 + 5 \cdot 6}{\sqrt{3^2+2^2+5^2} \cdot \sqrt{4^2+3^2+6^2}} = \frac{48}{\sqrt{38} \cdot \sqrt{61}} \approx 0.995$
\end{itemize}

### Importância da Normalização

**Por que normalizar?**

Quando atributos têm escalas muito diferentes, aqueles com maior magnitude dominam o cálculo de distância[1].

**Exemplo sem normalização:**
\begin{itemize}
\item Atributo 1 (idade): varia de 18 a 70
\item Atributo 2 (salário): varia de 1000 a 50000
\end{itemize}

A distância será dominada pelo salário, tornando a idade quase irrelevante.

**Métodos de Normalização:**

\begin{enumerate}
\item \textbf{Normalização Min-Max:} $x' = \frac{x - min}{max - min}$ (escala para [0,1])

\item \textbf{Padronização (Z-score):} $x' = \frac{x - \mu}{\sigma}$ (média 0, desvio padrão 1)

\item \textbf{Distância Euclidiana Padronizada:}
$$d_{standardized}(i, j) = \sqrt{\sum_{k=1}^{p}\frac{(x_{ik} - x_{jk})^2}{\sigma_k^2}}$$
\end{enumerate}

### Escolha da Medida Apropriada

\begin{table}
\begin{tabular}{|l|l|}
\hline
\textbf{Tipo de Dado} & \textbf{Medida Recomendada} \\
\hline
Numérico contínuo & Euclidiana, Manhattan, Mahalanobis \\
\hline
Binário/Categórico & Hamming, Jaccard \\
\hline
Textual/Esparso & Cosseno, Jaccard \\
\hline
Misto (numérico + categórico) & Gower, combinações ponderadas \\
\hline
Com outliers & Manhattan, Mahalanobis \\
\hline
Alta dimensionalidade & Cosseno, correlação \\
\hline
\end{tabular}
\caption{Escolha de medidas de distância por tipo de dado}
\end{table}

### Aplicações em Mineração de Dados

**Clustering (Agrupamento):**
\begin{itemize}
\item K-means usa distância Euclidiana
\item DBSCAN usa distâncias customizáveis
\item Hierarchical clustering pode usar diversas métricas
\end{itemize}

**Classificação:**
\begin{itemize}
\item K-NN (K-Nearest Neighbors) usa distâncias para encontrar vizinhos mais próximos
\item Diferentes métricas podem melhorar drasticamente o desempenho
\end{itemize}

**Detecção de Anomalias:**
\begin{itemize}
\item Objetos com alta distância média aos demais são potenciais outliers
\item Distância de Mahalanobis é especialmente útil
\end{itemize}

**Sistemas de Recomendação:**
\begin{itemize}
\item Similaridade cosseno para recomendar produtos/conteúdos similares
\item Jaccard para análise de cestas de compras
\end{itemize}

## 10. Exploração Inicial dos Dados (EDA - Exploratory Data Analysis)

**Definição:** A Análise Exploratória de Dados (EDA) é uma abordagem para analisar conjuntos de dados visando resumir suas características principais, frequentemente usando gráficos estatísticos e métodos de visualização[4][5]. EDA é uma etapa fundamental que precede a modelagem formal, permitindo que os dados "falem" e revelem insights que podem não ser aparentes através de análises tradicionais.

### Objetivos da Exploração Inicial

**Principais metas da EDA:**

\begin{enumerate}
\item \textbf{Avaliar Qualidade dos Dados:} Identificar erros, valores ausentes, inconsistências e outliers que possam impactar análises posteriores[5].

\item \textbf{Compreender Estrutura dos Dados:} Entender a distribuição das variáveis, relações entre atributos e características gerais do conjunto de dados.

\item \textbf{Descobrir Padrões e Anomalias:} Revelar tendências ocultas, correlações inesperadas e comportamentos atípicos nos dados[4].

\item \textbf{Formular Hipóteses:} Gerar perguntas e hipóteses para investigação mais aprofundada baseadas nos padrões observados.

\item \textbf{Validar Pressupostos:} Verificar se os dados atendem às suposições necessárias para técnicas estatísticas específicas (normalidade, homocedasticidade, etc.)[5].

\item \textbf{Guiar Pré-processamento:} Identificar transformações necessárias (normalização, tratamento de missing values, codificação de variáveis).
\end{enumerate}

### Filosofia da EDA

**Diferença da abordagem clássica:**

\begin{itemize}
\item \textbf{Abordagem Clássica:} Começa com um modelo assumido, depois coleta dados para testá-lo
\item \textbf{EDA:} Deixa os dados sugerirem quais modelos podem ser apropriados
\end{itemize}

A EDA foi popularizada por John Tukey, que enfatizou a importância de visualizar dados de formas que tornem características interessantes aparentes, em vez de simplesmente aplicar testes estatísticos predefinidos[4].

### Tipos de Análise Exploratória

#### 1. Análise Univariada

**Definição:** Examina uma variável por vez, focando em sua distribuição, tendência central e dispersão[6].

**Métodos para variáveis numéricas:**

\begin{itemize}
\item \textbf{Estatísticas Descritivas:}
  \begin{itemize}
  \item Medidas de tendência central: média, mediana, moda
  \item Medidas de dispersão: desvio padrão, variância, amplitude, IQR
  \item Medidas de forma: assimetria (skewness), curtose
  \item Percentis: quartis, decis
  \end{itemize}

\item \textbf{Visualizações:}
  \begin{itemize}
  \item Histogramas: mostram distribuição de frequências
  \item Boxplots: revelam mediana, quartis e outliers
  \item Gráficos de densidade: versão suavizada do histograma
  \item Q-Q plots: avaliam normalidade comparando quantis observados vs teóricos
  \item Stem-and-leaf: mantém valores originais enquanto mostra distribuição
  \end{itemize}
\end{itemize}

**Exemplo prático:**

Para uma variável "idade de clientes":
\begin{itemize}
\item Média: 35 anos
\item Mediana: 33 anos
\item Desvio padrão: 12 anos
\item Amplitude: 18-78 anos
\item Histograma revela distribuição ligeiramente assimétrica à direita
\item Boxplot identifica 3 valores outliers acima de 70 anos
\end{itemize}

**Métodos para variáveis categóricas:**

\begin{itemize}
\item \textbf{Tabelas de Frequência:} Contagem e porcentagem de cada categoria
\item \textbf{Gráficos de Barras:} Visualização das frequências por categoria
\item \textbf{Gráficos de Pizza:} Mostram proporções (usar com cautela - barras são preferíveis)
\item \textbf{Gráficos de Pareto:} Ordenam categorias por frequência
\end{itemize}

#### 2. Análise Bivariada

**Definição:** Examina relações entre duas variáveis simultaneamente[6].

**Numérica vs Numérica:**

\begin{itemize}
\item \textbf{Scatterplots (Gráficos de Dispersão):} Visualizam relação entre duas variáveis contínuas
\item \textbf{Correlação:} Pearson (linear), Spearman (monotônica), Kendall
\item \textbf{Regressão Linear Simples:} Linha de melhor ajuste
\item \textbf{Scatterplot Smoothing:} Curvas suavizadas para identificar tendências não-lineares
\end{itemize}

**Exemplo:** Relação entre "anos de experiência" e "salário"
\begin{itemize}
\item Scatterplot mostra tendência crescente
\item Correlação de Pearson: r = 0.78 (forte correlação positiva)
\item Alguns outliers visíveis (salários muito altos/baixos para experiência)
\end{itemize}

**Categórica vs Categórica:**

\begin{itemize}
\item \textbf{Tabelas de Contingência (Crosstabs):} Frequências cruzadas
\item \textbf{Teste Chi-quadrado:} Avalia independência entre variáveis
\item \textbf{Gráficos de Barras Agrupadas ou Empilhadas}
\item \textbf{Mosaicplots:} Visualização de tabelas de contingência
\end{itemize}

**Numérica vs Categórica:**

\begin{itemize}
\item \textbf{Boxplots Lado a Lado:} Comparam distribuições entre grupos
\item \textbf{Violin Plots:} Combinam boxplot com densidade
\item \textbf{Testes estatísticos:} t-test, ANOVA, Kruskal-Wallis
\item \textbf{Média/Mediana por Grupo}
\end{itemize}

**Exemplo:** Salário por nível educacional
\begin{itemize}
\item Boxplots revelam que mediana salarial aumenta com escolaridade
\item Ensino Superior tem maior variabilidade salarial
\item ANOVA confirma diferença significativa entre grupos (p < 0.001)
\end{itemize}

#### 3. Análise Multivariada

**Definição:** Examina relações entre três ou mais variáveis simultaneamente[6].

**Métodos principais:**

\begin{itemize}
\item \textbf{Matriz de Correlação:} Heatmap mostrando correlações entre todos os pares de variáveis
\item \textbf{Pairplot (Scatter Matrix):} Grid de scatterplots para múltiplas variáveis
\item \textbf{Parallel Coordinates:} Visualização de múltiplas dimensões
\item \textbf{Gráficos 3D:} Scatterplots tridimensionais (usar com cautela - difíceis de interpretar)
\item \textbf{Análise de Componentes Principais (PCA):} Redução de dimensionalidade para visualização
\end{itemize}

### Técnicas e Ferramentas Específicas de EDA

#### 1. Análise de Valores Ausentes

\begin{itemize}
\item \textbf{Mapa de Missing:} Visualização da localização de valores ausentes
\item \textbf{Percentual de Ausência:} Por variável e por registro
\item \textbf{Padrão de Ausência:} Totalmente aleatório (MCAR), aleatório (MAR) ou não-aleatório (MNAR)
\item \textbf{Correlação de Ausência:} Variáveis que tendem a ter valores ausentes juntas
\end{itemize}

#### 2. Detecção de Outliers

**Métodos estatísticos:**
\begin{itemize}
\item \textbf{IQR Method:} Valores abaixo de Q1 - 1.5×IQR ou acima de Q3 + 1.5×IQR
\item \textbf{Z-score:} Valores com |z| > 3 são considerados outliers
\item \textbf{Modified Z-score:} Usa mediana e MAD, mais robusto
\item \textbf{Isolation Forest:} Algoritmo de machine learning para detecção
\end{itemize}

**Visualizações:**
\begin{itemize}
\item Boxplots mostram outliers claramente
\item Scatterplots revelam outliers bivariados
\item Histogramas identificam valores extremos
\end{itemize}

#### 3. Análise de Distribuições

**Avaliação de normalidade:**
\begin{itemize}
\item \textbf{Q-Q plots:} Comparam quantis observados vs teóricos
\item \textbf{Histogramas com curva normal sobreposta}
\item \textbf{Testes formais:} Shapiro-Wilk, Kolmogorov-Smirnov, Anderson-Darling
\item \textbf{Estatísticas de forma:} Skewness (assimetria) e kurtosis (achatamento)
\end{itemize}

**Transformações comuns para normalizar:**
\begin{itemize}
\item Logarítmica: $log(x)$ - para assimetria positiva
\item Raiz quadrada: $\sqrt{x}$ - para contagens
\item Box-Cox: $\frac{x^\lambda - 1}{\lambda}$ - transformação paramétrica ótima
\item Inversa: $\frac{1}{x}$ - para assimetria extrema
\end{itemize}

#### 4. Análise de Séries Temporais (quando aplicável)

\begin{itemize}
\item \textbf{Line plots:} Visualização de tendências ao longo do tempo
\item \textbf{Decomposição:} Separar tendência, sazonalidade e ruído
\item \textbf{Autocorrelação:} ACF e PACF plots
\item \textbf{Smoothing resistente:} Moving averages, LOESS
\end{itemize}

### Workflow Típico de EDA

**Sequência recomendada:**

\begin{enumerate}
\item \textbf{Inspeção inicial:}
  \begin{itemize}
  \item Dimensões do dataset (linhas × colunas)
  \item Tipos de dados de cada variável
  \item Primeiras e últimas linhas (head/tail)
  \item Estrutura geral
  \end{itemize}

\item \textbf{Resumo estatístico:}
  \begin{itemize}
  \item Estatísticas descritivas para todas as variáveis
  \item Identificação de valores ausentes
  \item Contagem de valores únicos em categóricas
  \end{itemize}

\item \textbf{Análise univariada:}
  \begin{itemize}
  \item Distribuição de cada variável
  \item Identificação de outliers
  \item Avaliação de normalidade
  \end{itemize}

\item \textbf{Análise bivariada:}
  \begin{itemize}
  \item Relações entre preditores e variável alvo
  \item Correlações entre preditores
  \item Identificação de multicolinearidade
  \end{itemize}

\item \textbf{Análise multivariada:}
  \begin{itemize}
  \item Matriz de correlação completa
  \item PCA para entender estrutura de dimensionalidade
  \item Identificação de grupos naturais
  \end{itemize}

\item \textbf{Documentação de insights:}
  \begin{itemize}
  \item Padrões descobertos
  \item Problemas identificados
  \item Transformações necessárias
  \item Hipóteses para investigação
  \end{itemize}
\end{enumerate}

### Ferramentas e Tecnologias para EDA

**Python:**
\begin{itemize}
\item \textbf{Pandas:} Manipulação e análise de dados (describe(), info(), value\_counts())
\item \textbf{NumPy:} Operações numéricas e estatísticas
\item \textbf{Matplotlib:} Visualizações básicas e customizáveis
\item \textbf{Seaborn:} Visualizações estatísticas de alto nível
\item \textbf{Plotly:} Gráficos interativos
\item \textbf{Pandas Profiling:} Relatórios automáticos de EDA
\item \textbf{Sweetviz/AutoViz:} EDA automatizada
\end{itemize}

**R:**
\begin{itemize}
\item \textbf{dplyr/tidyverse:} Manipulação de dados
\item \textbf{ggplot2:} Sistema de gráficos poderoso e flexível
\item \textbf{skimr:} Resumos rápidos de dados
\item \textbf{DataExplorer:} EDA automatizada
\end{itemize}

### Boas Práticas em EDA

\begin{enumerate}
\item \textbf{Seja Sistemático:} Siga um workflow estruturado, não pule etapas

\item \textbf{Documente Tudo:} Registre descobertas, decisões e transformações aplicadas

\item \textbf{Visualize Sempre:} Gráficos revelam padrões que estatísticas numéricas podem esconder

\item \textbf{Questione Anomalias:} Investigue valores incomuns - podem ser erros ou insights valiosos

\item \textbf{Itere:} EDA não é linear - descobertas levam a novas perguntas

\item \textbf{Conheça o Domínio:} Interpretações corretas requerem conhecimento do contexto

\item \textbf{Evite Conclusões Prematuras:} Correlação ≠ causação, padrões podem ser espúrios

\item \textbf{Comunique Claramente:} Visualizações devem contar uma história compreensível
\end{enumerate}

### Exemplos Práticos de Insights de EDA

**Caso 1 - E-commerce:**
\begin{itemize}
\item EDA revela que 80\% das vendas vêm de 20\% dos produtos (Princípio de Pareto)
\item Scatterplot mostra correlação negativa entre preço e quantidade vendida
\item Análise temporal identifica pico de vendas sempre às sextas-feiras
\item Boxplots revelam que clientes premium gastam 3x mais que regulares
\end{itemize}

**Caso 2 - Saúde:**
\begin{itemize}
\item Histograma de idade mostra distribuição bimodal (dois picos)
\item Matriz de correlação revela forte correlação entre IMC e pressão arterial
\item Análise de missing values mostra que 15\% dos pacientes não têm resultado de colesterol
\item Q-Q plot indica que glicemia não segue distribuição normal
\end{itemize}

**Caso 3 - Recursos Humanos:**
\begin{itemize}
\item Análise bivariada mostra que funcionários com mais de 5 anos têm menor taxa de saída
\item Gráfico de barras indica que departamento de TI tem maior rotatividade
\item Scatterplot revela outliers: alguns funcionários com salário muito acima da média
\item Crosstab mostra associação entre satisfação e permanência na empresa
\end{itemize}

### Importância da EDA no Processo de Mineração

**Por que EDA é crucial:**

\begin{enumerate}
\item \textbf{Previne Erros:} Identifica problemas de qualidade antes de treinar modelos complexos

\item \textbf{Economiza Tempo:} Revela rapidamente se dados são adequados para análise pretendida

\item \textbf{Orienta Feature Engineering:} Sugere transformações e criação de novos atributos

\item \textbf{Melhora Modelos:} Conhecimento dos dados leva a melhores decisões de modelagem

\item \textbf{Gera Insights Imediatos:} Muitas perguntas de negócio podem ser respondidas só com EDA

\item \textbf{Comunica Resultados:} Visualizações facilitam comunicação com stakeholders não-técnicos
\end{enumerate}

A EDA é frequentemente subestimada, mas representa uma das etapas mais valiosas de qualquer projeto de mineração de dados. Investir tempo adequado na exploração inicial dos dados é fundamental para o sucesso de análises subsequentes[5].

## 11. Características Gerais de Bases de Dados

### a) Dimensionalidade

**Definição:** Quantidade de atributos (colunas ou features) que os dados possuem. Alta dimensionalidade pode causar problemas conhecidos como "maldição da dimensionalidade".

**Exemplos:**

\begin{itemize}
\item Base de dados de clientes com 10 atributos (baixa dimensionalidade)
\item Imagem digital com 1000×1000 pixels = 1 milhão de dimensões (alta dimensionalidade)
\item Dados genômicos com milhares de genes medidos (altíssima dimensionalidade)
\end{itemize}

**Desafios:** Quanto maior a dimensionalidade, mais dados são necessários para evitar overfitting e mais complexos ficam os algoritmos.

### b) Esparsidade

**Definição:** Proporção de valores ausentes, nulos ou zeros nos dados em relação ao total de valores possíveis.

**Exemplos:**

\begin{itemize}
\item Matriz de filmes vs usuários onde maioria não avaliou a maioria dos filmes (alta esparsidade)
\item Base de transações onde apenas alguns produtos são comprados (esparsa)
\item Dados de sensores onde muitos períodos não têm leituras (esparsos)
\end{itemize}

**Impacto:** Dados esparsos requerem técnicas especiais de tratamento e algoritmos adaptados.

### c) Resolução

**Definição:** Nível de detalhe ou granularidade temporal/espacial dos dados coletados.

**Exemplos:**

\begin{itemize}
\item Vendas agregadas por dia vs por hora vs por minuto (resolução temporal)
\item Localização por país vs por cidade vs por coordenadas GPS (resolução espacial)
\item Imagens com 100×100 pixels vs 1000×1000 pixels (resolução espacial)
\end{itemize}

**Trade-off:** Maior resolução = mais detalhes, mas também mais volume de dados e complexidade.

## 12. Qualidade dos Dados

A qualidade dos dados é fundamental para o sucesso da mineração. Problemas comuns incluem:

### a) Ruído

**Definição:** É o componente aleatório de um erro de medida. Problema resultante do processo de medição que pode envolver distorção de um valor ou adição de objetos "sujos" aos dados.

**Causas comuns:**

\begin{itemize}
\item Erros em sensores ou equipamentos de medição
\item Erros humanos durante entrada de dados
\item Interferências em transmissão de dados
\item Limitações de precisão dos instrumentos
\end{itemize}

**Exemplos práticos:**

\begin{itemize}
\item Sensor de temperatura registra 27°C em ambiente realmente a 24°C
\item Balança oscila entre 70.1 kg e 70.3 kg para mesma pessoa
\item Código de barras lido incorretamente por leitor defeituoso
\item Foto digital com "grãos" devido a baixa luminosidade
\end{itemize}

**Tratamento:**

\begin{itemize}
\item Técnicas de suavização (smoothing)
\item Filtros estatísticos (média móvel, filtros de mediana)
\item Binning (agrupamento de valores próximos)
\end{itemize}

### b) Outlier

**Definição:** São objetos de dados que, de alguma forma, têm características diferentes da maioria dos outros objetos, ou ainda valores de um atributo que não são comuns em relação aos valores típicos para aquele atributo.

**Tipos de outliers:**

\begin{itemize}
\item \textbf{Globais:} Desviam significativamente de todo o conjunto de dados
\item \textbf{Contextuais:} Anormais apenas em contexto específico
\item \textbf{Coletivos:} Conjunto de pontos que juntos formam anomalia
\end{itemize}

**Exemplos práticos:**

\begin{itemize}
\item Cliente faz saque de R\$ 100.000 quando média é R\$ 500
\item Pessoa com altura de 2,20m em população com média de 1,70m
\item Temperatura de 45°C registrada no inverno polar
\item Transação de cartão de crédito em país diferente poucos minutos após uso local
\end{itemize}

**Tratamento:**

\begin{itemize}
\item Investigar se é erro ou valor legítimo
\item Remover se for erro confirmado
\item Manter se representar fenômeno real importante
\item Usar algoritmos robustos a outliers
\end{itemize}

### c) Diferença entre Ruído e Outlier

\begin{table}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Aspecto} & \textbf{Ruído} & \textbf{Outlier} \\
\hline
Natureza & Erro aleatório de medição & Valor legítimo, mas atípico \\
\hline
Alcance & Afeta valores individuais com pequenas variações & Objeto/registro inteiro que destoa \\
\hline
Interesse & Geralmente indesejado & Pode ser interessante para análise \\
\hline
Tratamento & Remover ou suavizar & Investigar antes de decidir \\
\hline
Exemplo & Sensor oscila +/- 0.5°C & Cliente VIP com gastos 100x maiores \\
\hline
\end{tabular}
\caption{Comparação entre ruído e outlier}
\end{table}

### d) Ausência de Valores (Missing Values)

**Definição:** Falta de preenchimento de um ou mais valores em um ou mais atributos de registros.

**Tipos de ausência:**

\begin{itemize}
\item \textbf{MCAR (Missing Completely At Random):} Ausência totalmente aleatória, sem padrão
\item \textbf{MAR (Missing At Random):} Ausência relacionada a outros atributos observados
\item \textbf{MNAR (Missing Not At Random):} Ausência relacionada ao próprio valor ausente
\end{itemize}

**Exemplos práticos:**

\begin{itemize}
\item Cadastro de cliente sem data de nascimento informada
\item Exame médico não realizado em alguns pacientes
\item Perguntas de questionário deixadas em branco
\item Sensor desligado em determinados períodos
\end{itemize}

**Estratégias de Tratamento:**

\begin{enumerate}
\item \textbf{Eliminação:}
\begin{itemize}
\item Remover registros com valores ausentes (listwise deletion)
\item Remover atributos com muitas ausências (> 50-60\%)
\item Vantagem: simplicidade
\item Desvantagem: perda de informação
\end{itemize}

\item \textbf{Imputação Simples:}
\begin{itemize}
\item Substituir por média (atributos numéricos)
\item Substituir por mediana (mais robusta a outliers)
\item Substituir por moda (atributos categóricos)
\item Substituir por constante específica (ex: "Não informado")
\end{itemize}

\item \textbf{Imputação Avançada:}
\begin{itemize}
\item Regressão: estimar valores usando outros atributos
\item K-NN: usar valores de vizinhos mais próximos
\item Interpolação: para séries temporais
\item Multiple Imputation: gerar múltiplos datasets completos
\end{itemize}

\item \textbf{Manter valores ausentes:}
\begin{itemize}
\item Usar algoritmos que lidam nativamente com ausências (ex: XGBoost)
\item Tratar ausência como categoria especial
\end{itemize}
\end{enumerate}

**Escolha da estratégia:** Depende do tipo de ausência, proporção de dados faltantes, importância do atributo e algoritmo a ser usado.

## 13. Técnicas de Pré-processamento de Dados

### a) Agregação

**Definição:** Consiste na combinação de dois ou mais objetos (registros) ou valores em um único objeto ou valor mais representativo.

**Objetivos:**

\begin{itemize}
\item Reduzir volume de dados
\item Mudar escala ou granularidade de análise
\item Tornar dados mais estáveis e menos ruidosos
\end{itemize}

**Exemplos práticos:**

\begin{itemize}
\item Somar vendas diárias para formar vendas mensais ou anuais
\item Calcular média de notas de provas para obter nota final
\item Agregar transações individuais em totais por cliente
\item Combinar múltiplos sensores em leitura única mais confiável
\end{itemize}

**Operações comuns:** Soma, média, mediana, mínimo, máximo, contagem, desvio padrão.

### b) Amostragem

**Definição:** Seleção de um subconjunto representativo dos objetos de dados completos para ser analisado, quando trabalhar com todos os dados é inviável ou desnecessário.

**Tipos de amostragem:**

\begin{enumerate}
\item \textbf{Aleatória Simples:} Cada registro tem mesma probabilidade de ser selecionado
\item \textbf{Estratificada:} Mantém proporções de subgrupos importantes
\item \textbf{Sistemática:} Seleciona a cada k-ésimo elemento
\item \textbf{Por Conglomerados:} Seleciona grupos inteiros aleatoriamente
\end{enumerate}

**Exemplos:**

\begin{itemize}
\item Selecionar 10\% dos clientes para análise exploratória inicial
\item Amostrar 1000 transações de milhões para teste de algoritmo
\item Coletar dados de sensores a cada 5 minutos em vez de continuamente
\end{itemize}

**Vantagens:** Reduz custo computacional, acelera prototipagem, permite testes rápidos.

**Cuidados:** Amostra deve ser representativa para não introduzir viés.

### c) Redução de Dimensionalidade

**Definição:** Técnicas que reduzem a dimensionalidade (número de atributos) de uma base de dados criando novos atributos que são combinações matemáticas dos atributos originais, mantendo o máximo de informação relevante.

**Principais técnicas:**

\begin{enumerate}
\item \textbf{PCA (Principal Component Analysis):} Encontra direções de máxima variância
\item \textbf{SVD (Singular Value Decomposition):} Decomposição matricial para redução
\item \textbf{t-SNE e UMAP:} Para visualização em 2D/3D
\item \textbf{Autoencoders:} Redes neurais para compressão não-linear
\end{enumerate}

**Exemplos:**

\begin{itemize}
\item Reduzir 100 variáveis correlacionadas a 10 componentes principais
\item Comprimir imagens de alta resolução mantendo características essenciais
\item Visualizar dados de 50 dimensões em gráfico 2D
\end{itemize}

**Benefícios:** Reduz overfitting, acelera algoritmos, facilita visualização, remove redundância.

### d) Seleção de Subconjunto de Features (Feature Selection)

**Definição:** Seleção de um subconjunto dos atributos originais que são mais relevantes para a tarefa, descartando os menos importantes ou redundantes.

**Diferença de Redução de Dimensionalidade:** Mantém atributos originais intactos (não cria combinações), facilitando interpretação.

**Métodos principais:**

\begin{enumerate}
\item \textbf{Filter:} Avalia atributos independentemente do modelo (correlação, teste chi-quadrado)
\item \textbf{Wrapper:} Avalia subconjuntos usando o próprio modelo (forward/backward selection)
\item \textbf{Embedded:} Seleção integrada ao treinamento do modelo (Lasso, Random Forest importance)
\end{enumerate}

**Exemplos:**

\begin{itemize}
\item Selecionar 20 genes mais relevantes dentre 20.000 para diagnóstico
\item Escolher 5 indicadores financeiros mais importantes para prever inadimplência
\item Identificar variáveis meteorológicas mais preditivas de vendas de sorvete
\end{itemize}

### e) Criação de Características (Feature Engineering)

**Definição:** Processo de criar novos atributos mais informativos a partir dos dados originais, usando conhecimento do domínio e criatividade.

**Tipos:**

\begin{enumerate}
\item \textbf{Extração de Características:}
\begin{itemize}
\item Criar novos atributos diretamente dos dados brutos
\item Exemplos:
  \begin{itemize}
  \item Extrair "ano", "mês", "dia da semana" de uma data
  \item Calcular "idade" a partir de "data de nascimento"
  \item Extrair "domínio" de endereço de email
  \item Contar número de palavras em texto
  \end{itemize}
\end{itemize}

\item \textbf{Mapeamento para Novo Espaço:}
\begin{itemize}
\item Aplicar transformações matemáticas nos dados
\item Exemplos:
  \begin{itemize}
  \item Transformada de Fourier em séries temporais com ruído (isola frequências)
  \item Conversão de coordenadas cartesianas para polares
  \item Transformação logarítmica para dados exponenciais
  \end{itemize}
\end{itemize}

\item \textbf{Construção de Características:}
\begin{itemize}
\item Combinar múltiplos atributos existentes
\item Exemplos:
  \begin{itemize}
  \item Criar "densidade" a partir de "massa" e "volume"
  \item Calcular "IMC" de peso e altura
  \item Gerar "taxa de conversão" de visitas e compras
  \item Criar "renda per capita" de renda total e número de moradores
  \end{itemize}
\end{itemize}
\end{enumerate}

**Importância:** Feature engineering é frequentemente o diferencial entre modelos medianos e excelentes, pois incorpora conhecimento do domínio.

### f) Discretização

**Definição:** Processo de transformar atributos contínuos em categóricos (discretos), dividindo o intervalo contínuo em faixas ou bins.

**Métodos de discretização:**

\begin{enumerate}
\item \textbf{Largura igual:} Divide intervalo em bins de mesma amplitude
\item \textbf{Frequência igual:} Cada bin contém mesmo número de exemplos
\item \textbf{Baseada em clusters:} Usa algoritmos de agrupamento
\item \textbf{Supervisionada:} Considera variável alvo (ex: ChiMerge, MDLP)
\end{enumerate}

**Exemplos práticos:**

\begin{itemize}
\item Transformar idade contínua (0-100) em faixas etárias: "criança", "adolescente", "adulto", "idoso"
\item Converter salário em categorias: "baixo", "médio", "alto"
\item Discretizar temperatura em "frio", "morno", "quente"
\item Agrupar scores de crédito em níveis de risco
\end{itemize}

**Vantagens:** Simplifica modelos, torna dados mais robustos a outliers, facilita interpretação.

**Desvantagens:** Perda de informação, escolha de pontos de corte pode ser arbitrária.

### g) Binarização

**Definição:** Processo de transformar atributos contínuos e discretos em atributos binários (0 ou 1, verdadeiro ou falso).

**Tipos:**

\begin{enumerate}
\item \textbf{Threshold:} Define ponto de corte (ex: idade > 18? sim=1, não=0)
\item \textbf{One-Hot Encoding:} Transforma categórico em múltiplas variáveis binárias
\item \textbf{Presença/Ausência:} Indica se valor está presente
\end{enumerate}

**Exemplos práticos:**

\begin{itemize}
\item "Sexo masculino?" → sim=1, não=0
\item "Cliente inadimplente?" → sim=1, não=0
\item "Temperatura > 30°C?" → sim=1, não=0
\item Transformar "cor" (vermelho, azul, verde) em: "é\_vermelho=1/0", "é\_azul=1/0", "é\_verde=1/0"
\end{itemize}

**Uso:** Necessário para muitos algoritmos que só aceitam entrada numérica.

### h) Transformação de Variáveis

**Definição:** Refere-se a transformações matemáticas aplicadas a todos os valores de um atributo para alterar sua distribuição ou escala.

**Principais transformações:**

\begin{table}
\begin{tabular}{|l|l|l|}
\hline
\textbf{Transformação} & \textbf{Fórmula} & \textbf{Quando usar} \\
\hline
Logarítmica & $y = \log(x)$ ou $y = \log(x+1)$ & Dados assimétricos, distribuições exponenciais \\
\hline
Raiz quadrada & $y = \sqrt{x}$ & Contagens, dados positivos assimétricos \\
\hline
Box-Cox & $y = \frac{x^\lambda - 1}{\lambda}$ & Normalizar distribuições \\
\hline
Normalização & $y = \frac{x - min}{max - min}$ & Escalar para [0,1] \\
\hline
Padronização & $y = \frac{x - \mu}{\sigma}$ & Escalar para média 0, desvio 1 \\
\hline
\end{tabular}
\caption{Principais transformações de variáveis}
\end{table}

**Exemplos práticos:**

\begin{itemize}
\item Aplicar log em salários para reduzir assimetria
\item Normalizar notas de 0-100 para 0-1
\item Padronizar altura e peso para mesma escala antes de calcular distâncias
\item Box-Cox para transformar dados em distribuição mais próxima da normal
\end{itemize}

**Objetivos:** Atender pressupostos de algoritmos, melhorar convergência, equalizar importância de variáveis.

## 14. Classificação de Dados - Algoritmos Supervisionados

**Definição:** A classificação é uma tarefa preditiva de mineração de dados que consiste em atribuir objetos (registros, exemplos) a categorias pré-definidas chamadas classes, com base em seus atributos[7]. É um dos métodos mais fundamentais e amplamente utilizados em aprendizado de máquina supervisionado.

### Características da Classificação

**Natureza supervisionada:**
\begin{itemize}
\item Requer dados rotulados (exemplos com classes conhecidas) para treinamento
\item Aprende padrões que distinguem as classes
\item Generaliza conhecimento para classificar novos exemplos não vistos
\end{itemize}

**Componentes principais:**

\begin{enumerate}
\item \textbf{Conjunto de treinamento:} Dados rotulados usados para construir o modelo
\item \textbf{Atributos preditores:} Características (features) usadas para fazer predições
\item \textbf{Variável alvo (classe):} Categoria que queremos prever
\item \textbf{Modelo/Classificador:} Função que mapeia atributos → classe
\item \textbf{Conjunto de teste:} Dados rotulados usados para avaliar o modelo
\end{enumerate}

**Tipos de classificação:**

\begin{itemize}
\item \textbf{Binária:} Duas classes possíveis (ex: spam/não-spam, doente/saudável)
\item \textbf{Multiclasse:} Três ou mais classes mutuamente exclusivas (ex: espécies de íris: setosa/versicolor/virginica)
\item \textbf{Multi-label:} Múltiplas classes não-exclusivas (ex: tags de artigos: política, economia, sociedade)
\end{itemize}

### Processo de Classificação

**Fluxo geral:**

\begin{enumerate}
\item \textbf{Coleta e preparação dos dados}
\begin{itemize}
\item Reunir conjunto de dados rotulados
\item Realizar pré-processamento e limpeza
\item Dividir em conjuntos de treino e teste (tipicamente 70/30 ou 80/20)
\end{itemize}

\item \textbf{Treinamento do modelo}
\begin{itemize}
\item Aplicar algoritmo de classificação aos dados de treino
\item Ajustar hiperparâmetros (quando aplicável)
\item Modelo aprende padrões que distinguem as classes
\end{itemize}

\item \textbf{Avaliação}
\begin{itemize}
\item Testar modelo em dados não vistos (conjunto de teste)
\item Calcular métricas de desempenho (acurácia, precisão, recall, F1-score)
\item Analisar matriz de confusão
\end{itemize}

\item \textbf{Predição}
\begin{itemize}
\item Usar modelo treinado para classificar novos exemplos
\item Aplicar em produção para casos reais
\end{itemize}
\end{enumerate}

### K-Nearest Neighbors (K-NN) - Algoritmo Baseado em Instâncias

#### Princípio Fundamental

**Definição:** K-NN é um algoritmo de classificação não-paramétrico baseado em instâncias que classifica um novo exemplo com base na classe majoritária dos K exemplos mais próximos (vizinhos) no espaço de atributos[8].

**Premissa:** Exemplos semelhantes (próximos no espaço de atributos) tendem a pertencer à mesma classe - "diga-me com quem andas e te direi quem és".

**Características principais:**
\begin{itemize}
\item \textbf{Lazy learning:} Não constrói modelo explícito durante treinamento - apenas armazena os dados
\item \textbf{Instance-based:} Faz predições comparando diretamente com exemplos de treinamento
\item \textbf{Não-paramétrico:} Não assume forma específica da função de decisão
\item \textbf{Simples e intuitivo:} Fácil de entender e implementar
\end{itemize}

#### Algoritmo K-NN Passo a Passo

**Fase de Treinamento:**
\begin{enumerate}
\item Armazenar todos os exemplos de treinamento (pares atributos-classe)
\item Selecionar valor de K (número de vizinhos a considerar)
\item Escolher medida de distância (tipicamente Euclidiana)
\end{enumerate}

**Fase de Predição (para novo exemplo X):**
\begin{enumerate}
\item Calcular distância de X para todos os exemplos de treinamento
\item Selecionar os K exemplos mais próximos (menores distâncias)
\item Identificar as classes desses K vizinhos
\item Atribuir a X a classe mais frequente entre os K vizinhos (voto majoritário)
\end{enumerate}

#### Exemplo Detalhado de K-NN

**Cenário:** Classificar se um paciente tem diabetes com base em glicose e IMC.

**Dados de treinamento:**

\begin{table}
\begin{tabular}{|c|c|c|}
\hline
\textbf{Glicose} & \textbf{IMC} & \textbf{Diabetes?} \\
\hline
85 & 22 & Não \\
90 & 24 & Não \\
150 & 35 & Sim \\
160 & 38 & Sim \\
95 & 26 & Não \\
155 & 36 & Sim \\
\hline
\end{tabular}
\end{table}

**Novo paciente:** Glicose = 145, IMC = 33

**Com K=3:**

\begin{enumerate}
\item \textbf{Calcular distâncias Euclidianas:}

Para o primeiro exemplo (85, 22):
$$d = \sqrt{(145-85)^2 + (33-22)^2} = \sqrt{3600 + 121} = 61.0$$

Distâncias para todos os exemplos:
\begin{itemize}
\item (85, 22, Não): 61.0
\item (90, 24, Não): 55.9
\item (150, 35, Sim): 5.4
\item (160, 38, Sim): 16.2
\item (95, 26, Não): 51.5
\item (155, 36, Sim): 11.2
\end{itemize}

\item \textbf{Selecionar 3 vizinhos mais próximos:}
\begin{itemize}
\item (150, 35, Sim) - distância 5.4
\item (155, 36, Sim) - distância 11.2
\item (160, 38, Sim) - distância 16.2
\end{itemize}

\item \textbf{Voto majoritário:}
\begin{itemize}
\item 3 vizinhos com diabetes
\item 0 vizinhos sem diabetes
\end{itemize}

\item \textbf{Predição:} Paciente classificado como tendo diabetes
\end{enumerate}

#### Escolha do Valor de K

**Impacto do parâmetro K:**

\begin{itemize}
\item \textbf{K muito pequeno (ex: K=1):}
  \begin{itemize}
  \item Sensível a ruído e outliers
  \item Fronteiras de decisão irregulares
  \item Risco de overfitting (memorização dos dados de treino)
  \item Boa para dados limpos e bem separados
  \end{itemize}

\item \textbf{K muito grande (ex: K=N):}
  \begin{itemize}
  \item Muito suavizado, pode ignorar padrões locais
  \item Fronteiras de decisão excessivamente simples
  \item Risco de underfitting (modelo muito simplista)
  \item Classes minoritárias podem ser sempre ignoradas
  \end{itemize}

\item \textbf{K ideal (tipicamente ímpar):}
  \begin{itemize}
  \item Balanceia bias e variância
  \item K ímpar evita empates em classificação binária
  \item Valores comuns: 3, 5, 7, 11
  \item Determinar via validação cruzada
  \end{itemize}
\end{itemize}

**Exemplo visual do efeito de K:**

Para K=1:
\begin{itemize}
\item Novo exemplo é classificado pela classe do vizinho mais próximo
\item Decisão pode mudar drasticamente com pequenas variações
\end{itemize}

Para K=5:
\begin{itemize}
\item Novo exemplo considera 5 vizinhos mais próximos
\item Decisão mais estável e menos sensível a outliers
\end{itemize}

#### Medidas de Distância em K-NN

**1. Distância Euclidiana (padrão):**

$$d_{Euclidean}(x, y) = \sqrt{\sum_{i=1}^{n}(x_i - y_i)^2}$$

Apropriada para atributos numéricos contínuos em escalas similares.

**2. Distância Manhattan:**

$$d_{Manhattan}(x, y) = \sum_{i=1}^{n}|x_i - y_i|$$

Útil quando atributos têm escalas muito diferentes ou quando movimentos são restritos a eixos.

**3. Distância de Minkowski (generalização):**

$$d_{Minkowski}(x, y) = \left(\sum_{i=1}^{n}|x_i - y_i|^p\right)^{1/p}$$

Parâmetro p controla sensibilidade (p=1: Manhattan, p=2: Euclidiana).

**4. Distância de Hamming:**

Para atributos categóricos - conta quantos atributos diferem.

#### K-NN Ponderado

**Problema:** Todos os K vizinhos têm mesmo peso no voto, mesmo que um esteja muito mais próximo que outros.

**Solução:** K-NN ponderado atribui pesos aos vizinhos baseados na distância.

**Fórmula:**

$$peso_i = \frac{1}{d_i}$$

ou

$$peso_i = \frac{1}{d_i^2}$$

onde $d_i$ é a distância ao i-ésimo vizinho.

**Exemplo:**

Para K=3 com distâncias [5.4, 11.2, 16.2]:
\begin{itemize}
\item Pesos: [1/5.4, 1/11.2, 1/16.2] = [0.185, 0.089, 0.062]
\item Vizinho mais próximo tem